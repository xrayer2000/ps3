{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30caa51",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c755f1cc11c29a05288c56443b7c6b32",
     "grade": false,
     "grade_id": "import_np_and_gym",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67400a04",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c955111bc44a1c1b71b639aa1512210",
     "grade": false,
     "grade_id": "into_viz_env",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The code below visulizes the environment with random actions, the goal will be to create a policy that chooses actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdd054d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "536981e95a9d74d3c9fa5eca9b86dc8c",
     "grade": false,
     "grade_id": "viz_taxi",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env_render = gym.make(\"Taxi-v3\", render_mode='human') # create environment with rendering \n",
    "state, _ = env_render.reset() # initiate the environment\n",
    "done = False\n",
    "i = 0\n",
    "while not done:\n",
    "    action = env_render.action_space.sample()  # Samples random actions from our environment. \n",
    "    new_state, reward, terminated, truncated, _ = env_render.step(action) # takes as step in our environment\n",
    "    done = terminated or truncated # checks if we have reached our goal or timed out\n",
    "    state = new_state # updates the current state.\n",
    "    if i > 20: # End after 20 steps\n",
    "        done = True\n",
    "    i+=1\n",
    "env_render.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61cd03c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8ea50bf67faa3a737f798a0fc60ac14c",
     "grade": false,
     "grade_id": "intro2_q_learning",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The Q-learning algorithm\n",
    "\n",
    "In Q-learning we want to learn a look-up table that for every state and action has the future discounted reward. It is initialized with zeros. \n",
    "\n",
    "| State  | a1  | a2  | a3  | a4 |\n",
    "|---|---|---|---|---|\n",
    "| 1  | 0  | 0  | 0 | 0  |\n",
    "| 2  | 0  | 0  | 0 | 0  |\n",
    "| ...  | ...  | ...  | ... | ... |\n",
    "| N  | 0  | 0  | 0 | 0  |\n",
    "\n",
    "The training then updates this table to the correct values. It might look something like:\n",
    "\n",
    "| State  | a1  | a2  | a3  | a4 |\n",
    "|---|---|---|---|---|\n",
    "| 1  | 0.1  | -5  | 1 | 0.4  |\n",
    "| 2  | -1  | -2  | 0 | 4  |\n",
    "| ...  | ...  | ...  | ... | ... |\n",
    "| N  | 1  | 2  | -2 | 1  |\n",
    "\n",
    "List of some terms in the feild of Q-learning:\n",
    "* State - A state in our environment is a representation of all the information available to make a decision. At time step \"t,\" we have the state \"s_t.\" In the case of the taxi problem, there are 500 different states. Each state is an integer within the range of 0 to 500, and it represents: \n",
    "    * 25 different taxi locations\n",
    "    * 5 passenger locations\n",
    "    * 4 possible destinations\n",
    "* Markov decision process (MDP) - Q-learning is an algorithm designed to solve problems involving finite discrete Markov decision processes (MDPs) https://en.wikipedia.org/wiki/Markov_decision_process. \n",
    "* Action - The actions is how we transition between states, at time step $t$ the action $a_t$ takes us to $s_{t+1}$. In this case we have 6 possible actions:\n",
    "    * 0: Move south (down)\n",
    "    * 1: Move north (up)\n",
    "    * 2: Move east (right)\n",
    "    * 3: Move west (left)\n",
    "    * 4: Pickup passenger\n",
    "    * 5: Drop off passenger\n",
    "* Reward - In reinforcement learning, the system provides rewards and penalties to guide the learning process. When the actor successfully completes a task or makes a correct move, a reward $r$ is given. Conversely, penalties can be imposed for suboptimal or incorrect actions. These rewards and penalties serve as the means to influence and shape what the algorithm attempts to learn and optimize. The algorithm aims to maximize the cumulative rewards over time by making decisions that lead to higher overall rewards and fewer penalties.\n",
    "* Q-table - The q table contains for every state (s) and action (a) the expected future discounted reward. Often written as Q(s, a), see above.\n",
    "* Policy - How do we choose our action, Q(s, a) outputs how good a action is but not directly which action to choose. \n",
    "* Greedy policy - For a state choose the action that has the highest expected reward.\n",
    "* Epsilon greedy policy - During training allow for exploration by having a probability of choosing a random action instead of greedy. Exploration is necessary for learning, without it the learning can get stuck in the same solution. \n",
    "* Episode - One espisode is the agient interacting with the environment from start to finish or termination i.e. a timeseries of states and actions.  \n",
    "\n",
    "The q-table can be initated with zeros. It can be updated as:\n",
    "\n",
    "$Q(s_t, a_t) = (1- \\alpha)Q(s_t, a_t) + \\alpha(r_t + \\gamma \\underbrace{\\max}_{a} Q(s_{t+1}, a))$\n",
    "\n",
    "where $\\alpha$ is the learning rate and $\\gamma$ is the dicount factor. For training, we want a policy that both explores new paths while at the same time exploits the current best solution. The solution is an epsilon greedy policy. There is more information at https://en.wikipedia.org/wiki/Q-learning. Also notice that we never update the Q values for the goal state, as the episode will terminate upon reaching the goal. The epsilon greedy policy:\n",
    "\n",
    "$a \\begin{cases} random & \\textrm{With probability } \\epsilon \\\\\n",
    "                greedy &  \\textrm{With probability } 1 - \\epsilon \\end{cases}$\n",
    "                \n",
    "Where $\\epsilon$ is ussually decresing with traning, here is exponential decay. \n",
    "\n",
    "$ \\epsilon = \\epsilon_{\\textrm{min}} + (\\epsilon_{\\textrm{max}} -\\epsilon_{\\textrm{min}}) e^{- \\lambda i_e}$\n",
    "\n",
    "where $\\epsilon_{\\textrm{min}}$ is the minimum value of $\\epsilon$, i.e. we allways want some exploration, otherwise we wont learn anything new. $\\epsilon_{\\textrm{max}}$ usually set to $1$, how much exploration to start of with. $\\lambda$ is the decay rate, how fast should we decreese the amount of exploration. $i_e$ is how many episodes we have trained on.\n",
    "\n",
    "Next we can check how many state and actions our environment has. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97796a3b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd3889eb32f96a65d7c4bfad86a85b75",
     "grade": true,
     "grade_id": "check_action_states",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "print('Number of states', num_states, 'and number of actions', num_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bceba74",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0340ba367da67b9b3010c5861815e210",
     "grade": false,
     "grade_id": "inro_hyp_q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We define our hyper parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19044479",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b62a4ae2ee04105502c6d27b261b287",
     "grade": true,
     "grade_id": "hyper_parame_q_lr",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1 # aplha in our q-learning update\n",
    "discount_factor = 0.99 # gamma\n",
    "epsilon = 1.0 # epsilon\n",
    "epsilon_max = 1.0 # epsilon max\n",
    "epsilon_min = 0.01 # epsilon min\n",
    "epsilon_lambda = 0.01 # lambda\n",
    "num_episodes = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f27928",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "85d19353fc769c3d25cd0047363a45db",
     "grade": false,
     "grade_id": "intro_ex_1_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exersice 1.1: Q-learning \n",
    "\n",
    "Here the task is to implement the Q-learning algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55f8373",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c9c27f4ca9a0e2cfff4b11d71d6d84a",
     "grade": false,
     "grade_id": "q_learning",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Initiate q-table\n",
    "\n",
    "q_table = np.array(np.zeros([num_states, num_actions]))\n",
    "print(q_table);\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Calculate action with exploration-exploitation trade-off\n",
    "        \n",
    "        # TODO\n",
    "        if(random.uniform(0, 1) < epsilon):\n",
    "            action = random.randint(0, num_actions - 1)\n",
    "        else:\n",
    "            action = np.argmax(q_table[state, :])\n",
    "     \n",
    "        # step \n",
    "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # TODO\n",
    "        # Q-value update\n",
    "        total_reward = (1-learning_rate)*q_table[state, action] + learning_rate*(reward + discount_factor * np.argmax(q_table[new_state,:]))\n",
    "        \n",
    "        \n",
    "        \n",
    "        q_table[state, action] += total_reward \n",
    "        state = new_state\n",
    "\n",
    "    # TODO\n",
    "    # Decay exploration probability (epsilon)\n",
    "    epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-epsilon_lambda * episode)\n",
    "    \n",
    "    print(f\"Episode: {episode+1}, Total Reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fdf225",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d7ae88e0a68128cf258b2e24a0d248a",
     "grade": false,
     "grade_id": "intro_ex_1_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exersice 1.2: Evaluate Q-learning \n",
    "\n",
    "Here implement greedy actions only with the learned Q-table. It is also possible to visualize the results. A coorectly implemented q-learning algorithm should get average reward of higher than 7.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331b4952",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "976a35458ed027a03623d60c836f8a23",
     "grade": true,
     "grade_id": "test_q_learning",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the trained policy\n",
    "env_render = gym.make(\"Taxi-v3\", render_mode='human') # env with renderig, if you want to visualize, but it is slow. \n",
    "\n",
    "total_rewards = []\n",
    "num_evaluation_episodes = 100\n",
    "\n",
    "for _ in range(num_evaluation_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # TODO\n",
    "        # Choose greedy action \n",
    "        \n",
    "        # action = \n",
    "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        #new_state, reward, terminated, truncated, _ = env_render.step(action) # uncomment for visualization\n",
    "\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        state = new_state\n",
    "    \n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "average_reward = np.mean(total_rewards)\n",
    "print(f\"Average Reward over {num_evaluation_episodes} evaluation episodes: {average_reward}\")\n",
    "env_render.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da7579c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55f00d503598087056fa0ba55af905ae",
     "grade": false,
     "grade_id": "Q_learning_ex_1_3_text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exersice 1.3: Q-learning question\n",
    "\n",
    "Are there any limitations with Q-learning, what type of problems can it solve and what type of problems is it less suitable for?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9736e3af",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "02eae55556f1aca2a5ad2bc313f1af22",
     "grade": true,
     "grade_id": "Q_learning_ex_1_3_ans",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42a88d69",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c53f8558b5868bbfc34d33a2c63fbc37",
     "grade": false,
     "grade_id": "Deep_q_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Deep Q learning\n",
    "\n",
    "In this task, we'll tackle the challenge of balancing an inverted pendulum. What makes this problem intriguing is that it involves continuous input variables, making the traditional Q-learning approach with a finite table impractical. Instead, we'll use a neural network to approximate the Q-table, enabling us to efficiently handle continuous input while still producing discrete outputs.\n",
    "\n",
    "First we will construct our neural network, this will be done in three steps. First we will define our activation function, then we will define a what a layer is and lastly we will construct the network.  \n",
    "\n",
    "First let us go over the math of a neural network. They should be seen as general function approximator. A segment of neural network can be represented with a graph representation, see image below. \n",
    "\n",
    "<img src=\"imgs/NN_ps3.png\" width=\"800\"/>\n",
    "\n",
    "\n",
    "\n",
    "### Naming\n",
    "* $a^i_j$ is the output of neuron j in layer i.\n",
    "* $b^i_j$ is the sum of the input to neuron j in layer i.\n",
    "* $g()$ is the activation function of a neuron (the last layer does not have a activation funciton in this case)\n",
    "* $w^i_{jk}$ is the weights of layer i going from neron k in layer i-1 to neuron j in layer i. \n",
    "\n",
    "### Forward propagation\n",
    "\n",
    "A neural network can be seen as a function $y = f(x)$ where $x$ is an input vector and $y$ is an output vector. Forward propagation describes how we go from the input $x$ to the output $y$. Each layer can be caluculated in a sequence, where we start from the input. \n",
    "\n",
    "First each neuron $j$ in layer 1 calculated the sum of the input to that neuron:\n",
    "\n",
    "$b^1_j = \\sum_{k=1}^{len(x)}( w^1_{j,k}x_k) + w^1_{j,0}$ \n",
    "\n",
    "The output of the first layer is caluclated but using the activation funciton. \n",
    "\n",
    "$a^1_j = g(b^1_j)$\n",
    "\n",
    "More generally for layer $i$ and with matrix multiplication we can write this as.\n",
    "\n",
    "$b^i = w^i \\begin{bmatrix} 1 \\\\ a^{i-1}\\end{bmatrix}$\n",
    "\n",
    "where $b^i$ is a vector that has the same dimension as number of neurons in layer $i$ and the 1 added to the input is for calculating the bias. \n",
    "\n",
    "$a^i = g(b^i)$\n",
    "\n",
    "This is then caluculated sequentially until the last layer has been reashed. For the last layer without activation function we would have\n",
    "\n",
    "$\\hat{y} = a^i = b^i$\n",
    " \n",
    "### Backward propagation\n",
    "\n",
    "The way we train a network is by updating the weights $w$, they are the parameters we want to estimate to solve the problem. The way we are going to update the weights is through gradient descent. The gradients are calculated sequentially from the last layer to the first. The reason for this is that this allows us to leverage the chain rule, this saves us a lot of computational complexity. \n",
    "\n",
    "Calculating gradients:\n",
    "\n",
    "For each parameter (weight) in the neural network we will calculate a gradient:\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial w^i_{jk}}$ \n",
    "where $\\mathcal{L(\\mathbf{y}, \\hat{\\mathbf{y}})}$ is the loss function, which we want to minimize. A normal loss function could for example be the mean square error $\\mathcal{L} = \\frac{1}{n}\\sum_{i=1}^n(y-\\hat{y})^2$. The gradients can be caluclated with \n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial w^i_{jk}} = \\Delta^i_{j} a^{i-1}_k$\n",
    "\n",
    "where $\\Delta^i_j$ can be seen as the \"precived\" error for the input to neuron $j$ in layer $i$. It can be calculated with the chain rule. Let $n(i)$ be a function that returns the number of neuron of layer $i$.\n",
    "\n",
    "$\\Delta^i_j = \\left( \\sum_{r=1}^{n(i+1)} \\Delta^{i+1}_r w^{i+1}_{rj} \\right) \\frac{\\partial g(b^i_j)}{\\partial b^i_j}$\n",
    "\n",
    "For the first itteration there where is no $\\Delta^{i+1}_r$ then they are calculated as:\n",
    "\n",
    "$\\Delta^i_j = \\frac{\\partial \\mathcal{L(\\mathbf{y}, \\hat{\\mathbf{y}})}}{\\partial \\hat{y}_j}$\n",
    "\n",
    "This is under the assumption that there is no activation on the last layer. The nice thing is that the calculation of the vector $\\Delta^i$ can be done as a matrix multiplication:\n",
    "\n",
    "$\\Delta^i = (\\bar{w}^{i+1})^T \\Delta^{i+1} \\odot \\nabla_{b^i} g(b^i)$\n",
    "\n",
    "where $\\odot$ is elementwise multiplication, $\\bar{w}$ is the weight without bias and $\\nabla_{b^i} g(b^i) = \\begin{bmatrix} \\frac{\\partial g(b^i_1)}{\\partial b^i_1} &...&\\frac{\\partial g(b^i_n)}{\\partial b^i_n} \\end{bmatrix}^T$.\n",
    "\n",
    "We can then calculate the gradients for a layer with:\n",
    "\n",
    "$\\nabla_{w^i} \\mathcal{L} = \\Delta^i  (a^{i-1})^T$\n",
    "\n",
    "We are then going to update the weights with:\n",
    "\n",
    "$w^i \\leftarrow w^i - \\alpha(\\nabla_{w^i} \\mathcal{L} + \\lambda w^i)$\n",
    "\n",
    "where $\\alpha$ is the learning rate and $\\lambda$ is a regularization term (L2 regularization), we don't want the weight to become too large. The regularization also combats overfitting to some extent. We will come back to $\\nabla_{w^i}$ later. \n",
    "\n",
    "For more on the chain rule see the exersice \"NN_exersice\" on canvas under the Exercises folder.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66120cf7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c730103a194f266df1be32674e9ba3f4",
     "grade": false,
     "grade_id": "intro_ex_2_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exersice 2.1: ReLu\n",
    "\n",
    "Here we will implement a class for the ReLu activation function. The activation function is what allows deep neural networks to express non-linearities. The class will only have two functions, forward(x) and diff(x). The forward function takes a vector or matrix and outputs the same shape where for every element we have applied the ReLu function. The diff function returns the partial derivative of the ReLu function. This will later be used in our neural network. \n",
    "\n",
    "Hint: np.where() is useful function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a37fbd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f35eaa602fefa4e6297bcd504abfb7f",
     "grade": false,
     "grade_id": "ReLu",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ReLu():\n",
    "    def forward(self, x):\n",
    "        # x is a numpy array and can have any shape. \n",
    "        # calculates the output of the ReLU function\n",
    "        # TODO\n",
    "        return = \n",
    "        \n",
    "        \n",
    "    def diff(self, x):\n",
    "        # x is a numpy array and can have any shape.\n",
    "        # calculates the partial differential of the ReLU function\n",
    "        # TODO\n",
    "        # return = \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473050f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7ae97291c36dea27bb0214ce4a430be",
     "grade": true,
     "grade_id": "ReLU_test",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "relu = ReLu()\n",
    "\n",
    "input = np.array([[1.2],[-1.1], [0.0], [10.3]])\n",
    "out = relu.forward(input)\n",
    "ref = np.array([[1.2],[0.0], [0.0], [10.3]])\n",
    "print('forward test: ', np.array_equal(out, ref))\n",
    "\n",
    "diff_out = relu.diff(input)\n",
    "diff_ref = np.array([[1.],[0.0], [0.0], [1]])\n",
    "print('diff test: ', np.array_equal(diff_out, diff_ref))\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb8933",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "feca8b4e21fb40ef49d69a6e665223a9",
     "grade": false,
     "grade_id": "intor_ex_2_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exersice 2.2: Layer\n",
    "\n",
    "A dense neural network is constructed by layers of neurons, here we will define a class to represent our layers. It will contain the weights and activation function for the layer. The layer class will contain two functions, forward and backward.\n",
    "\n",
    "Hint: it is a good practice to return a copy of np arrays or matrices as otherwise modifying the output can modify the internally saved matrix as it is the same object, these problems are not easy to debug. For a matrix A, then return A.copy(). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b5b7d0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b63bce90e62ae9d46fca75f8b887e988",
     "grade": false,
     "grade_id": "Layer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, num_in, num_out, activation_func=None):\n",
    "        self.weights = np.random.rand(num_out, num_in+1)-0.5 # (to layer, from layer) \n",
    "        self.gradients = np.zeros((num_out, num_in+1))\n",
    "        self.a_input = None  # placeholder for saving the input to the forward function\n",
    "        self.b = None  # placeholder for saving the value right before the activation function\n",
    "        self.activation_func = activation_func  # class instance of activation function \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        # x is a standing vector\n",
    "        # Add a 1 for the bias term on the input, and save this in the self.a_input, this will be used later in backward()\n",
    "        # Calculate b, the summation in each neuron. Can be done as a matrix multiplication, save in self.b. \n",
    "        # Check if the activation function is None, otherwise run the forward of the activation function. \n",
    "        # return the output of the layer\n",
    "        \n",
    "        \n",
    "    \n",
    "    def backward(self, pre_delta, pre_w=None):\n",
    "        # TODO \n",
    "        # pre_delta is from layer i+1, pre_w is the weights from layer i+1.\n",
    "        # First check it pre_w is None, it is none for the last layer (output layer).\n",
    "        # Calculate this layers delta \n",
    "        # Caluclate this layers gradients and save to self.gradients (do not use the gradients to update the weights yet)\n",
    "        # return delta from this layer \n",
    "        # Hint: see calculations above under Deep Q Learning\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a75cb0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40e594185c2d06f7dff5d08df6281c78",
     "grade": false,
     "grade_id": "Introduction",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Reinforcement learning and deep-learning\n",
    "\n",
    "## General outline for this assingment \n",
    "\n",
    "* Q-learning, openai taxi problem \n",
    "* Deep Q-learning, control inverted pendelum\n",
    "* Image classification with pytorch for MNIST dataset\n",
    "\n",
    "This assignment combines the elements of comprehension and implementation. You will be responsible for coding in some sections, while in others, we will provide you with code to study.\n",
    "\n",
    "### Q-Learning\n",
    "\n",
    "In this task, you will employ Q-learning to tackle decision-making in a grid world game. This will provide you with a foundational grasp of Q-learning and the principles of reinforcement learning.\n",
    "\n",
    "To set up the environment for the first two tasks, you'll need to install the required software. https://pypi.org/project/gymnasium/\n",
    "\n",
    "pip install gymnasium==0.29.1\n",
    "\n",
    "\n",
    "### Deep Q-Learning\n",
    "\n",
    "Deep Q-learning combines neural networks with Q-learning. In this task, you're embarking on a rather ambitious journey. Your goal is to construct a neural network from the ground up and we will show how to apply it to a reinforcement learning problem. This endeavor aims to lay a solid foundation for your comprehension of neural networks and the inherent challenges they pose. This may well be one of the very few opportunities you'll have to create such a network from scratch, granting you insight into what transpires behind the scenes. Following this, you'll likely rely on libraries like PyTorch or TensorFlow, which abstract much of the mathematics. \n",
    "\n",
    "The core concept behind deep Q-learning involves approximating the Q-table with a neural network. However, as you'll soon discover, this endeavor is far from straightforward.\n",
    "\n",
    "\n",
    "### CNN + dense with pytorch\n",
    "\n",
    "To wrap up, you will employ PyTorch to tackle an image classification problem, specifically the classification of handwritten numbers. In this segment, we will demonstrate how PyTorch can be used to construct neural networks. Your objective is to enhance the accuracy of this model to reach a minimum of 98%.\n",
    "\n",
    "You can easily install pytorch through (it is enough with just cpu support, if you have an nvidia graphics card you can try the cuda installation), https://pytorch.org/\n",
    "\n",
    "I'm running pytorch version 2.0.1, newer versions should hopefully also work.\n",
    "\n",
    "I have also only tested this assignment with numpy version 1.xx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31d1c61",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31aeb295b3527a6c938eba211934afed",
     "grade": false,
     "grade_id": "Intro_q_learning",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Q-Learning\n",
    "\n",
    "In reinforcement learning, the primary aim is to maximize future rewards. While this may not seem significantly different from supervised learning, the implications are profound. Consider the scenario where you want an AI to navigate a maze: at each time step, the AI can make a move in any valid direction. If you were to approach this as a regression problem, akin to supervised learning, you would need to know the precise solution for every step, which becomes infeasible for complex problems. Reinforcement learning simplifies the human side of the problem by using rewards. For instance, you can reward the AI only when it successfully solves the maze and allow the AI to explore freely. You don't instruct it on how to solve the maze directly but instead provide a reward for completing the task. The AI will then determine the most effective way to maximize the reward over the entire episode. \n",
    "\n",
    "If you want a robot to learn how to walk, you would provide rewards for moving in the correct direction and penalties if it falls, but you wouldn't explicitly instruct it on how to walk. This is the advantage of reinforcement learning. \n",
    "\n",
    "Q-learning represents one of the simplest versions of reinforcement learning.   \n",
    "\n",
    "In this task, we will employ Q-learning to tackle the taxi problem, which is further described in https://gymnasium.farama.org/environments/toy_text/taxi/ and visualization is provided a few cells down. Short summary: \n",
    "\n",
    "\n",
    ">There are four designated pick-up and drop-off locations (Red, Green, Yellow and Blue) in the 5x5 grid world. The taxi starts off at a random square and the passenger at one of the designated locations.\n",
    "The goal is move the taxi to the passenger’s location, pick up the passenger, move to the passenger’s desired destination, and drop off the passenger. Once the passenger is dropped off, the episode ends.\n",
    "The player receives positive rewards for successfully dropping-off the passenger at the correct location. Negative rewards for incorrect attempts to pick-up/drop-off passenger and for each step where another reward is not received.\n",
    "\n",
    "Let's start by importing the necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67400a04",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c955111bc44a1c1b71b639aa1512210",
     "grade": false,
     "grade_id": "into_viz_env",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The code below visulizes the environment with random actions, the goal will be to create a policy that chooses actions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61cd03c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8ea50bf67faa3a737f798a0fc60ac14c",
     "grade": false,
     "grade_id": "intro2_q_learning",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The Q-learning algorithm\n",
    "\n",
    "In Q-learning we want to learn a look-up table that for every state and action has the future discounted reward. It is initialized with zeros. \n",
    "\n",
    "| State  | a1  | a2  | a3  | a4 |\n",
    "|---|---|---|---|---|\n",
    "| 1  | 0  | 0  | 0 | 0  |\n",
    "| 2  | 0  | 0  | 0 | 0  |\n",
    "| ...  | ...  | ...  | ... | ... |\n",
    "| N  | 0  | 0  | 0 | 0  |\n",
    "\n",
    "The training then updates this table to the correct values. It might look something like:\n",
    "\n",
    "| State  | a1  | a2  | a3  | a4 |\n",
    "|---|---|---|---|---|\n",
    "| 1  | 0.1  | -5  | 1 | 0.4  |\n",
    "| 2  | -1  | -2  | 0 | 4  |\n",
    "| ...  | ...  | ...  | ... | ... |\n",
    "| N  | 1  | 2  | -2 | 1  |\n",
    "\n",
    "List of some terms in the feild of Q-learning:\n",
    "* State - A state in our environment is a representation of all the information available to make a decision. At time step \"t,\" we have the state \"s_t.\" In the case of the taxi problem, there are 500 different states. Each state is an integer within the range of 0 to 500, and it represents: \n",
    "    * 25 different taxi locations\n",
    "    * 5 passenger locations\n",
    "    * 4 possible destinations\n",
    "* Markov decision process (MDP) - Q-learning is an algorithm designed to solve problems involving finite discrete Markov decision processes (MDPs) https://en.wikipedia.org/wiki/Markov_decision_process. \n",
    "* Action - The actions is how we transition between states, at time step $t$ the action $a_t$ takes us to $s_{t+1}$. In this case we have 6 possible actions:\n",
    "    * 0: Move south (down)\n",
    "    * 1: Move north (up)\n",
    "    * 2: Move east (right)\n",
    "    * 3: Move west (left)\n",
    "    * 4: Pickup passenger\n",
    "    * 5: Drop off passenger\n",
    "* Reward - In reinforcement learning, the system provides rewards and penalties to guide the learning process. When the actor successfully completes a task or makes a correct move, a reward $r$ is given. Conversely, penalties can be imposed for suboptimal or incorrect actions. These rewards and penalties serve as the means to influence and shape what the algorithm attempts to learn and optimize. The algorithm aims to maximize the cumulative rewards over time by making decisions that lead to higher overall rewards and fewer penalties.\n",
    "* Q-table - The q table contains for every state (s) and action (a) the expected future discounted reward. Often written as Q(s, a), see above.\n",
    "* Policy - How do we choose our action, Q(s, a) outputs how good a action is but not directly which action to choose. \n",
    "* Greedy policy - For a state choose the action that has the highest expected reward.\n",
    "* Epsilon greedy policy - During training allow for exploration by having a probability of choosing a random action instead of greedy. Exploration is necessary for learning, without it the learning can get stuck in the same solution. \n",
    "* Episode - One espisode is the agient interacting with the environment from start to finish or termination i.e. a timeseries of states and actions.  \n",
    "\n",
    "The q-table can be initated with zeros. It can be updated as:\n",
    "\n",
    "$Q(s_t, a_t) = (1- \\alpha)Q(s_t, a_t) + \\alpha(r_t + \\gamma \\underbrace{\\max}_{a} Q(s_{t+1}, a))$\n",
    "\n",
    "where $\\alpha$ is the learning rate and $\\gamma$ is the dicount factor. For training, we want a policy that both explores new paths while at the same time exploits the current best solution. The solution is an epsilon greedy policy. There is more information at https://en.wikipedia.org/wiki/Q-learning. Also notice that we never update the Q values for the goal state, as the episode will terminate upon reaching the goal. The epsilon greedy policy:\n",
    "\n",
    "$a \\begin{cases} random & \\textrm{With probability } \\epsilon \\\\\n",
    "                greedy &  \\textrm{With probability } 1 - \\epsilon \\end{cases}$\n",
    "                \n",
    "Where $\\epsilon$ is ussually decresing with traning, here is exponential decay. \n",
    "\n",
    "$ \\epsilon = \\epsilon_{\\textrm{min}} + (\\epsilon_{\\textrm{max}} -\\epsilon_{\\textrm{min}}) e^{- \\lambda i_e}$\n",
    "\n",
    "where $\\epsilon_{\\textrm{min}}$ is the minimum value of $\\epsilon$, i.e. we allways want some exploration, otherwise we wont learn anything new. $\\epsilon_{\\textrm{max}}$ usually set to $1$, how much exploration to start of with. $\\lambda$ is the decay rate, how fast should we decreese the amount of exploration. $i_e$ is how many episodes we have trained on.\n",
    "\n",
    "Next we can check how many state and actions our environment has. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bceba74",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0340ba367da67b9b3010c5861815e210",
     "grade": false,
     "grade_id": "inro_hyp_q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We define our hyper parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f27928",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "85d19353fc769c3d25cd0047363a45db",
     "grade": false,
     "grade_id": "intro_ex_1_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exersice 1.1: Q-learning \n",
    "\n",
    "Here the task is to implement the Q-learning algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fdf225",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d7ae88e0a68128cf258b2e24a0d248a",
     "grade": false,
     "grade_id": "intro_ex_1_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exersice 1.2: Evaluate Q-learning \n",
    "\n",
    "Here implement greedy actions only with the learned Q-table. It is also possible to visualize the results. A coorectly implemented q-learning algorithm should get average reward of higher than 7.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da7579c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55f00d503598087056fa0ba55af905ae",
     "grade": false,
     "grade_id": "Q_learning_ex_1_3_text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exersice 1.3: Q-learning question\n",
    "\n",
    "Are there any limitations with Q-learning, what type of problems can it solve and what type of problems is it less suitable for?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a88d69",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c53f8558b5868bbfc34d33a2c63fbc37",
     "grade": false,
     "grade_id": "Deep_q_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Deep Q learning\n",
    "\n",
    "In this task, we'll tackle the challenge of balancing an inverted pendulum. What makes this problem intriguing is that it involves continuous input variables, making the traditional Q-learning approach with a finite table impractical. Instead, we'll use a neural network to approximate the Q-table, enabling us to efficiently handle continuous input while still producing discrete outputs.\n",
    "\n",
    "First we will construct our neural network, this will be done in three steps. First we will define our activation function, then we will define a what a layer is and lastly we will construct the network.  \n",
    "\n",
    "First let us go over the math of a neural network. They should be seen as general function approximator. A segment of neural network can be represented with a graph representation, see image below. \n",
    "\n",
    "<img src=\"imgs/NN_ps3.png\" width=\"800\"/>\n",
    "\n",
    "\n",
    "\n",
    "### Naming\n",
    "* $a^i_j$ is the output of neuron j in layer i.\n",
    "* $b^i_j$ is the sum of the input to neuron j in layer i.\n",
    "* $g()$ is the activation function of a neuron (the last layer does not have a activation funciton in this case)\n",
    "* $w^i_{jk}$ is the weights of layer i going from neron k in layer i-1 to neuron j in layer i. \n",
    "\n",
    "### Forward propagation\n",
    "\n",
    "A neural network can be seen as a function $y = f(x)$ where $x$ is an input vector and $y$ is an output vector. Forward propagation describes how we go from the input $x$ to the output $y$. Each layer can be caluculated in a sequence, where we start from the input. \n",
    "\n",
    "First each neuron $j$ in layer 1 calculated the sum of the input to that neuron:\n",
    "\n",
    "$b^1_j = \\sum_{k=1}^{len(x)}( w^1_{j,k}x_k) + w^1_{j,0}$ \n",
    "\n",
    "The output of the first layer is caluclated but using the activation funciton. \n",
    "\n",
    "$a^1_j = g(b^1_j)$\n",
    "\n",
    "More generally for layer $i$ and with matrix multiplication we can write this as.\n",
    "\n",
    "$b^i = w^i \\begin{bmatrix} 1 \\\\ a^{i-1}\\end{bmatrix}$\n",
    "\n",
    "where $b^i$ is a vector that has the same dimension as number of neurons in layer $i$ and the 1 added to the input is for calculating the bias. \n",
    "\n",
    "$a^i = g(b^i)$\n",
    "\n",
    "This is then caluculated sequentially until the last layer has been reashed. For the last layer without activation function we would have\n",
    "\n",
    "$\\hat{y} = a^i = b^i$\n",
    " \n",
    "### Backward propagation\n",
    "\n",
    "The way we train a network is by updating the weights $w$, they are the parameters we want to estimate to solve the problem. The way we are going to update the weights is through gradient descent. The gradients are calculated sequentially from the last layer to the first. The reason for this is that this allows us to leverage the chain rule, this saves us a lot of computational complexity. \n",
    "\n",
    "Calculating gradients:\n",
    "\n",
    "For each parameter (weight) in the neural network we will calculate a gradient:\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial w^i_{jk}}$ \n",
    "where $\\mathcal{L(\\mathbf{y}, \\hat{\\mathbf{y}})}$ is the loss function, which we want to minimize. A normal loss function could for example be the mean square error $\\mathcal{L} = \\frac{1}{n}\\sum_{i=1}^n(y-\\hat{y})^2$. The gradients can be caluclated with \n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial w^i_{jk}} = \\Delta^i_{j} a^{i-1}_k$\n",
    "\n",
    "where $\\Delta^i_j$ can be seen as the \"precived\" error for the input to neuron $j$ in layer $i$. It can be calculated with the chain rule. Let $n(i)$ be a function that returns the number of neuron of layer $i$.\n",
    "\n",
    "$\\Delta^i_j = \\left( \\sum_{r=1}^{n(i+1)} \\Delta^{i+1}_r w^{i+1}_{rj} \\right) \\frac{\\partial g(b^i_j)}{\\partial b^i_j}$\n",
    "\n",
    "For the first itteration there where is no $\\Delta^{i+1}_r$ then they are calculated as:\n",
    "\n",
    "$\\Delta^i_j = \\frac{\\partial \\mathcal{L(\\mathbf{y}, \\hat{\\mathbf{y}})}}{\\partial \\hat{y}_j}$\n",
    "\n",
    "This is under the assumption that there is no activation on the last layer. The nice thing is that the calculation of the vector $\\Delta^i$ can be done as a matrix multiplication:\n",
    "\n",
    "$\\Delta^i = (\\bar{w}^{i+1})^T \\Delta^{i+1} \\odot \\nabla_{b^i} g(b^i)$\n",
    "\n",
    "where $\\odot$ is elementwise multiplication, $\\bar{w}$ is the weight without bias and $\\nabla_{b^i} g(b^i) = \\begin{bmatrix} \\frac{\\partial g(b^i_1)}{\\partial b^i_1} &...&\\frac{\\partial g(b^i_n)}{\\partial b^i_n} \\end{bmatrix}^T$.\n",
    "\n",
    "We can then calculate the gradients for a layer with:\n",
    "\n",
    "$\\nabla_{w^i} \\mathcal{L} = \\Delta^i  (a^{i-1})^T$\n",
    "\n",
    "We are then going to update the weights with:\n",
    "\n",
    "$w^i \\leftarrow w^i - \\alpha(\\nabla_{w^i} \\mathcal{L} + \\lambda w^i)$\n",
    "\n",
    "where $\\alpha$ is the learning rate and $\\lambda$ is a regularization term (L2 regularization), we don't want the weight to become too large. The regularization also combats overfitting to some extent. We will come back to $\\nabla_{w^i}$ later. \n",
    "\n",
    "For more on the chain rule see the exersice \"NN_exersice\" on canvas under the Exercises folder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a37fbd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f35eaa602fefa4e6297bcd504abfb7f",
     "grade": false,
     "grade_id": "ReLu",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ReLu():\n",
    "    def forward(self, x):\n",
    "        # x is a numpy array and can have any shape. \n",
    "        # calculates the output of the ReLU function\n",
    "        # TODO\n",
    "        return = \n",
    "        \n",
    "        \n",
    "    def diff(self, x):\n",
    "        # x is a numpy array and can have any shape.\n",
    "        # calculates the partial differential of the ReLU function\n",
    "        # TODO\n",
    "        # return = \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb8933",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "feca8b4e21fb40ef49d69a6e665223a9",
     "grade": false,
     "grade_id": "intor_ex_2_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exersice 2.2: Layer\n",
    "\n",
    "A dense neural network is constructed by layers of neurons, here we will define a class to represent our layers. It will contain the weights and activation function for the layer. The layer class will contain two functions, forward and backward.\n",
    "\n",
    "Hint: it is a good practice to return a copy of np arrays or matrices as otherwise modifying the output can modify the internally saved matrix as it is the same object, these problems are not easy to debug. For a matrix A, then return A.copy(). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d87bfea",
   "metadata": {},
   "source": [
    "### Test your layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e711df",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "920a556408026ecfb4d90de73a9813e3",
     "grade": false,
     "grade_id": "intro_ex_2_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2.3: Define the neural network\n",
    "We will create a neural network that has two hidden layers with 64 neurons each with ReLU activation function and the output layer has the same size as the number of outputs but with no activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8f1388",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "43db34785f0b137c3676b30c6d8e8ebb",
     "grade": false,
     "grade_id": "intro_test_supervized",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Test supervised learning to see if the network performes as expeced. \n",
    "Here we want to test the neural network on a simple supervised problem, it should learn the output of two functions. Does the y_est follow the true y? You should see that they follow each other fairly well, usually within 0.1 difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ddac8d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e7fa1d9f6c48b2664a612b83e35de017",
     "grade": false,
     "grade_id": "intro_deep_q_lr",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Training Deep Q-learning\n",
    "\n",
    "The idea for this section is that we will walk you through the code and the process and then you will answer some theoretical questions.\n",
    "\n",
    "Now, we have a neural network, and we aim to employ it to balance an inverted pendulum on a cart. This problem involves continuous input space, with four inputs, and a discrete output space, consisting of two possible actions (left or right). The reward function for this task is straightforward - we receive a reward for every step taken, with the objective being to maintain balance. The task is considered unsuccessful if the pendulum leans too far or the cart moves excessively to the side. You can find more details about this problem at https://gymnasium.farama.org/environments/classic_control/cart_pole/.\n",
    "\n",
    "The challenge with this problem is that the continuous input space makes it impractical to use standard Q-learning, as the Q-table would become excessively large. To address this, we'll replace the Q-table with a neural network. However, as we will demonstrate, this transition is not without its challenges. \n",
    "\n",
    "The primary challenge in deep Q-learning lies in training stability, as the training process tends to be noisy, and many issues can arise. To enhance stability, we introduce three key components: target networks, experience replay, and batch gradient descent. However, before delving into those specifics, let's discuss the fundamental principle of replacing the Q-table with a neural network.\n",
    "\n",
    "In Q-learning, we maintain a table of future expected rewards, and for a given state and action, we can obtain the value, denoted as $Q(s, a)$. Our objective is to approximate this Q-value using a neural network, denoted as $f$, which takes the state as input and produces a value for each possible action, i.e., $\\hat{q} = f(s)$, where $\\hat{q}$ is a vector. The training loop remains similar to traditional Q-learning, but in this case, we formulate a loss function. \n",
    "\n",
    "$\\mathcal{L}(s_t, a) = (q(s_t)_a -\\hat{q}(s_t)_a)^2$\n",
    "\n",
    "were \n",
    "\n",
    "$q(s_t)_a = r + \\gamma \\underbrace{\\max}_{a} \\hat{q}(s_{t+1})$ \n",
    "\n",
    "and $\\hat{q}(s_t)_a$ is the output of the network for action $a$ and state $s$, and $\\hat{q}(s_{t+1})$ is for the next state. This will make the network converge towarde the Q-table, atleast theoretically. The problem is that this is prone to instabilities. We can no longer initiate the network to predict zeros initially, so the initial predictions of the future reward will be completely wrong. Every time we update the weights we will also change $\\underbrace{\\max}_{a} \\hat{q}(s_{t+1})$, which means that the value the network tries to converge to is allways changing, and if it varies to fast instabilities will arrise. One other of the downsides of using gradient decent is that it can easily forget thing that it trained on a long time ago. The methods below attempts to minimize these issues.     \n",
    "\n",
    "\n",
    "### Target network \n",
    "The idea here is to make $\\underbrace{\\max}_{a} \\hat{q}(s_{t+1})$ a slow varying target to increase stability. We introdue a second network called the target network. So we have two netorks $\\hat{q} = f(s)$ and $\\hat{q}_T = f_T(s)$, where the second is the target network. We use the second network to estimate the future estimated reward.  \n",
    "\n",
    "$q(s_t, a) = r + \\gamma \\underbrace{\\max}_{a} \\hat{q}_T(s_{t+1}, a)$ \n",
    "\n",
    "The idea is then to update the target network slowly towards the first network, the weights of the target network $w_T$ can then be updated with \n",
    "\n",
    "$w_T = w_T + \\alpha_T (w - w_T)$\n",
    "\n",
    "where $\\alpha_T$ is the target update rate. This adds stability to the traning as the future expected reward is more steady and slow changing. \n",
    "\n",
    "### Experiance replay\n",
    "\n",
    "Next up is experience replay, because of gradient descent we don't want to train on samples that are too close to each other as there is a risk that the network will forget things it has learned. The idea is then in the training loop to not directly train on the observed data but instead save it in a buffer/queue. For training we sample for the queue, this means that the network might train on old data or new data. This minimizes that it forgets or that a lot of samples in sequence are too similar. \n",
    "\n",
    "### Batch gradient decent \n",
    "\n",
    "To furter imporove stability we sample multiple timesteps from our memory buffer, then we caluclate the gradients for each of them and take the average of the gradients. It is then those average graidentes we use as $\\nabla_{w^i}$ to update the weights, this minimize noise as each update of the weights is based on multiple data samples. \n",
    "\n",
    "We will now show how these things can be implemented in code. \n",
    "\n",
    "## Hyper parameters\n",
    "\n",
    "Neural networks can have quite a few hyper parameters and it can be quite tricky to tune them. I have provided some hypter parameters that work, but feel free to tune them if desiered.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934c2d25",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bb03be0a51b48ea2d81562815144a44b",
     "grade": false,
     "grade_id": "intro_train_function",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Training function\n",
    "\n",
    "Here we define our training function, this is what will be called for every step in our trianing loop. This part is responsible for sampling a batch from the memory buffer and calculating the gradients for the batch, it is also here we have our reward function and update our network. Lastly we will update the weights of the target network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5640e56",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04be1614b924470e3fe908540536f664",
     "grade": false,
     "grade_id": "intro_train_loop",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##  Training loop\n",
    "\n",
    "Here is the actual training loop that runs the simulation. This part should look similar to the q-learning training loop at the beginning of the assignment. The difference is the call of the training function and appending to the memory buffer. Because of the unstable nature of deep Q-learning, you typically save the network that performed the best during training. It can sometimes take more than 100 episodes before you see any noticeable learning.\n",
    "\n",
    "Even with all the additions of the target network, batch gradient decent and experience replay, the training can still be unstable. It is therefore common in reinforcement learning to always save the best-performing network. \n",
    "\n",
    "## Exercise 2.4: Epsilon greedy - deep learning\n",
    "Fill code for action and update of epsilon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7077e413",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0e1fd35486a4811743b16a28bb47542",
     "grade": false,
     "grade_id": "title_learnng_curve",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Plot the learning curve\n",
    "Below we plot the total reward againt the episodes, you can see that the learning is noisy and quite often it completly forgets everything after 300-400 episodes. The highest score is 500 and it is usually posible to reach it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f11611",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a8b7bed97956ea11cbbe7104071d4db",
     "grade": false,
     "grade_id": "title_learned_policy",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Test the learned policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044dca8c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "019c66ab67b3a12e2e6a0db85860b6f4",
     "grade": false,
     "grade_id": "ex_2_4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exersice 2.5: \n",
    "\n",
    "1. What are the similarities between Q-learning and deep Q-learning?\n",
    "2. What are the differences and what implications does these difference have? Are Q-learning and deep Q-learning equally good at the same tasks or are they suited for different problems? \n",
    "3. Are there any difficulties with deep Q-learning?\n",
    "4. What conclusion can you draw from the the learning curve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732320bc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "283b1e53de966ca8b05df62817159256",
     "grade": false,
     "grade_id": "Intro-CNN_pytorch",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# CNN and pytorch\n",
    "\n",
    "## Exersice 3.1\n",
    "\n",
    "Now that we've gained a solid understanding of how neural networks function and how they are trained, we'll shift our focus to tackling an image classification problem. In this instance, we will utilize ready-made packages and libraries to streamline the process.\n",
    "\n",
    "Debugging neural networks is not an easy task! If you're looking for a helpful resource, the University of Amsterdam's course on debugging neural networks might provide valuable insights and techniques to address common challenges. https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/guide3/Debugging_PyTorch.html\n",
    "\n",
    "The starting initial code is based on https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
    "\n",
    "We are going to use the MNIST data set, which containts images of handwritten numbers and your task is to classify the written numbers from the images. \n",
    "\n",
    "Your task is to modify the code below to increase the accuracy to atleast 98%. There will hints written throughout below. Then in task 3.2 you will describe the changes you made and try to motivate why you made them.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71891f6b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e16ece813ff7e8be7579f03d5549528f",
     "grade": true,
     "grade_id": "CNN_imports",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ef6cbf",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6952f2383930e2fbacffa8d51413946d",
     "grade": true,
     "grade_id": "Get_mnist_data",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# visualize the data\n",
    "num_images_to_plot = 9\n",
    "fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
    "\n",
    "for i in range(num_images_to_plot):\n",
    "    image, label = training_data[i]\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    ax.imshow(image.squeeze(), cmap='gray')\n",
    "    ax.set_title(f\"Label: {label}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb266a78",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5e6463b9fd0ad8c7d337ba33cfda21e",
     "grade": true,
     "grade_id": "CNN_data_loaders",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Batch size, this is something you can tune, but the current value is decent.  \n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83db25ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7bc5151d90e0559a8e741a39fdb23e7",
     "grade": false,
     "grade_id": "intro_define_network",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Define the neural network\n",
    "\n",
    "Here there is alot of things that can be tuned, I do encurage to add convolutional layers. While it is not neccessary to achive the accuracy, the convolutional layers are very good at images. For example a convolution layer can be defnined as following\n",
    "\n",
    "self.conv1 = nn.Conv2d(1, 3, 5)\n",
    "\n",
    "where the first input is the number of in channels. For a grey image there is only one channel, for an rgb image there is 3 channels. The second input is the number of out channels, you can see it as parrallel filters. The last input is the filter size. More infromation exists at https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html. The thing to keep in mind is the dimensions out of convolution. In this case the inpout is 1x28x28 and the output 3x24x24.\n",
    "\n",
    "In the forward funciton it can be used as\n",
    "\n",
    "x =  F.relu(self.conv1(x))\n",
    "\n",
    "And lets say you have defined a linear (dense) layer \n",
    "\n",
    "self.fd1 = nn.Linear(3 * 24 * 24, 120)\n",
    "\n",
    "They can be combined in the forward function as:\n",
    "\n",
    "x =  F.relu(self.conv1(x))\n",
    "\n",
    "x = torch.flatten(x, 1) # this flattens the output to a vector\n",
    "\n",
    "x =  F.relu(self.fd1(x))\n",
    "\n",
    "\n",
    "A lot of things can be modified here, such as number of layers, the size of the layers, the type of layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e6f984",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7408262fd7e4d7f66589a8d873fc4081",
     "grade": true,
     "grade_id": "cnn_define_network",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# TODO, modify \n",
    "# Define the neural network model\n",
    "class NeuralNetwork_pytorch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fd1 = nn.Linear(28*28, 512) # dense layers (input, output), output is the number of neurons for the layer\n",
    "        self.fd2 = nn.Linear(512, 512)\n",
    "        self.fd3 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) \n",
    "        x = F.relu(self.fd1(x))\n",
    "        x = F.relu(self.fd2(x))\n",
    "        x = self.fd3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = NeuralNetwork_pytorch().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22193a02",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10998906721c2c93d54bfa413d8a0f22",
     "grade": false,
     "grade_id": "train_function_cnn",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Our traning function, nothing to modify here\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % (100*int(64/batch_size)) == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a59e70",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec1aaa765533f8acf154c2f063c84e51",
     "grade": false,
     "grade_id": "cnn_test_function",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# our test function, nothing to modify here.\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897fff6b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0afd422e93f815692478b30e4555d87",
     "grade": true,
     "grade_id": "Training_loop_cnn",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# The training loop, here you can change the optimizer, the opimizer is how you update the weights. SGD does \n",
    "# gradient decent. Adam optimizer adds some extra steps and regularization. \n",
    "\n",
    "epochs = 5 # try to not change the number of epochs, we want something that learns fast. \n",
    "           # One epoch trains through all the data. \n",
    "    \n",
    "model = NeuralNetwork_pytorch().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss() # you can change the loss function \n",
    "\n",
    "# you can change the optimizer. The optimizer is how you updates the weights and there are differnet ways to do that.\n",
    "# For more info see https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial4/Optimization_and_Initialization.html#Optimization\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a6d5da",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "108d76a3aad36c29ce5dbcda5c0af526",
     "grade": false,
     "grade_id": "cell-c4a405f59c1f4998",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exersice 3.2: modifications\n",
    "\n",
    "What modifications did you do and what improvments did you see from them. Can you reason to why they worked?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe273f73",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53f3838a7ac06254a76374891bea3a16",
     "grade": true,
     "grade_id": "cell-c08fcb694697cec5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Text answer: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
